{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(968, 110)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kagglegym\n",
    "\n",
    "# Create environment\n",
    "env = kagglegym.make()\n",
    "# Get first observation\n",
    "observation = env.reset()\n",
    "\n",
    "observation.features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[110, 36]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 5 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-2 # feel free to play with this to train faster or more stably.\n",
    "gamma = 0.99 # discount factor for reward\n",
    "\n",
    "D = observation.features.head().shape[1] # input dimensionality\n",
    "H = int(D / 3) # number of hidden layer neurons\n",
    "\n",
    "[D, H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#This defines the network as it goes from taking an observation of the environment to \n",
    "#giving a probability of chosing to the action of moving left or right.\n",
    "observations = tf.placeholder(tf.float32, [None,D] , name=\"input_x\")\n",
    "W1 = tf.get_variable(\"W1\", shape=[D, H],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(observations,W1))\n",
    "W2 = tf.get_variable(\"W2\", shape=[H, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "score = tf.matmul(layer1,W2)\n",
    "probability = tf.nn.sigmoid(score)\n",
    "\n",
    "#From here we define the parts of the network needed for learning a good policy.\n",
    "tvars = tf.trainable_variables()\n",
    "input_y = tf.placeholder(tf.float32,[None,1], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32,name=\"reward_signal\")\n",
    "\n",
    "# The loss function. This sends the weights in the direction of making actions \n",
    "# that gave good advantage (reward over time) more likely, and actions that didn't less likely.\n",
    "#TODO fix loglik = tf.log(input_y*(input_y - probability) + (1 - input_y)*(input_y + probability))\n",
    "#TODO fix loss = -tf.reduce_mean(loglik * advantages) \n",
    "loss = -tf.reduce_mean((input_y - probability) * advantages)\n",
    "newGrads = tf.gradients(loss,tvars)\n",
    "\n",
    "# Once we have collected a series of gradients from multiple episodes, we apply them.\n",
    "# We don't just apply gradeients after every episode in order to account for noise in the reward signal.\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate) # Our optimizer\n",
    "# Placeholders to send the final gradients through when we update.\n",
    "W1Grad = tf.placeholder(tf.float32,name=\"batch_grad1\") \n",
    "W2Grad = tf.placeholder(tf.float32,name=\"batch_grad2\")\n",
    "#\n",
    "batchGrad = [W1Grad,W2Grad]\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad,tvars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    # in Python 2:\n",
    "    # for t in reversed(xrange(0, r.size)):\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Debug:\n",
    "    def __init__(self, state):\n",
    "        self.state = state\n",
    "\n",
    "    def log(self, msg):\n",
    "        if self.state % 300 == 0:\n",
    "            print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(968, 111)\n",
      "(959, 111)\n",
      "(974, 111)\n",
      "(967, 111)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>derived_0</th>\n",
       "      <th>derived_1</th>\n",
       "      <th>derived_2</th>\n",
       "      <th>derived_3</th>\n",
       "      <th>derived_4</th>\n",
       "      <th>fundamental_0</th>\n",
       "      <th>fundamental_1</th>\n",
       "      <th>fundamental_2</th>\n",
       "      <th>...</th>\n",
       "      <th>technical_36</th>\n",
       "      <th>technical_37</th>\n",
       "      <th>technical_38</th>\n",
       "      <th>technical_39</th>\n",
       "      <th>technical_40</th>\n",
       "      <th>technical_41</th>\n",
       "      <th>technical_42</th>\n",
       "      <th>technical_43</th>\n",
       "      <th>technical_44</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>811134</th>\n",
       "      <td>2154</td>\n",
       "      <td>910</td>\n",
       "      <td>0.074597</td>\n",
       "      <td>-0.015414</td>\n",
       "      <td>0.29208</td>\n",
       "      <td>0.343019</td>\n",
       "      <td>0.044199</td>\n",
       "      <td>-0.228965</td>\n",
       "      <td>0.179896</td>\n",
       "      <td>-0.157234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138343</td>\n",
       "      <td>-8.638552e-01</td>\n",
       "      <td>-1.679721e-15</td>\n",
       "      <td>-8.638552e-01</td>\n",
       "      <td>0.070375</td>\n",
       "      <td>-0.140491</td>\n",
       "      <td>-7.442452e-03</td>\n",
       "      <td>-1.562500e-02</td>\n",
       "      <td>-0.050378</td>\n",
       "      <td>-0.004286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811135</th>\n",
       "      <td>2155</td>\n",
       "      <td>910</td>\n",
       "      <td>0.020373</td>\n",
       "      <td>-0.053024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.014809</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.095301</td>\n",
       "      <td>-0.097290</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018055</td>\n",
       "      <td>-3.469200e-04</td>\n",
       "      <td>-3.469200e-04</td>\n",
       "      <td>-3.469200e-04</td>\n",
       "      <td>-0.075559</td>\n",
       "      <td>0.211429</td>\n",
       "      <td>-7.994175e-27</td>\n",
       "      <td>-3.330669e-16</td>\n",
       "      <td>-0.004531</td>\n",
       "      <td>-0.002657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811136</th>\n",
       "      <td>2156</td>\n",
       "      <td>910</td>\n",
       "      <td>-0.010002</td>\n",
       "      <td>-0.047333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.086585</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.307436</td>\n",
       "      <td>0.229720</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.300517</td>\n",
       "      <td>-9.636440e-17</td>\n",
       "      <td>-9.636440e-17</td>\n",
       "      <td>-9.636440e-17</td>\n",
       "      <td>0.075476</td>\n",
       "      <td>0.008847</td>\n",
       "      <td>8.564127e-01</td>\n",
       "      <td>-2.000000e+00</td>\n",
       "      <td>0.003307</td>\n",
       "      <td>-0.005547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  timestamp  derived_0  derived_1  derived_2  derived_3  \\\n",
       "811134  2154        910   0.074597  -0.015414    0.29208   0.343019   \n",
       "811135  2155        910   0.020373  -0.053024        NaN  -0.014809   \n",
       "811136  2156        910  -0.010002  -0.047333        NaN   0.086585   \n",
       "\n",
       "        derived_4  fundamental_0  fundamental_1  fundamental_2    ...     \\\n",
       "811134   0.044199      -0.228965       0.179896      -0.157234    ...      \n",
       "811135        NaN      -0.095301      -0.097290            NaN    ...      \n",
       "811136        NaN       0.307436       0.229720            NaN    ...      \n",
       "\n",
       "        technical_36  technical_37  technical_38  technical_39  technical_40  \\\n",
       "811134      0.138343 -8.638552e-01 -1.679721e-15 -8.638552e-01      0.070375   \n",
       "811135      0.018055 -3.469200e-04 -3.469200e-04 -3.469200e-04     -0.075559   \n",
       "811136     -0.300517 -9.636440e-17 -9.636440e-17 -9.636440e-17      0.075476   \n",
       "\n",
       "        technical_41  technical_42  technical_43  technical_44         y  \n",
       "811134     -0.140491 -7.442452e-03 -1.562500e-02     -0.050378 -0.004286  \n",
       "811135      0.211429 -7.994175e-27 -3.330669e-16     -0.004531 -0.002657  \n",
       "811136      0.008847  8.564127e-01 -2.000000e+00      0.003307 -0.005547  \n",
       "\n",
       "[3 rows x 111 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's an example of loading the CSV using Pandas's built-in HDF5 support:\n",
    "import pandas as pd\n",
    "\n",
    "with pd.HDFStore(\"../input/train.h5\", \"r\") as train:\n",
    "    # Note that the \"train\" dataframe is the only dataframe in the file\n",
    "    df = train.get(\"train\")\n",
    "\n",
    "print(df[df[\"timestamp\"] == 909].shape)\n",
    "print(df[df[\"timestamp\"] == 1000].shape)\n",
    "print(df[df[\"timestamp\"] == 1100].shape)\n",
    "\n",
    "t910 = df[df[\"timestamp\"] == 910]\n",
    "print(t910.shape)\n",
    "t910.tail(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extend(x, target):\n",
    "    keys = list(x.keys())\n",
    "    zeros = [.0] * len(keys)\n",
    "    rows = x.shape[0]\n",
    "    while rows < target:\n",
    "        tail = pd.DataFrame(dict(zip(keys, zeros)), index=[rows])\n",
    "        x = pd.concat([x, tail])\n",
    "        rows = x.shape[0]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradBuffer 2 110 36\n",
      "Timestamp #1000\n",
      "rewards.tail 956    0.025702\n",
      "957   -0.006563\n",
      "958    0.000643\n",
      "Name: y, dtype: float32\n",
      "Reward is -1\n",
      "y=0    0.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "Name: y, dtype: float32\n",
      "py=[[ -5.83076896e-03]\n",
      " [  8.23848459e-05]\n",
      " [  3.94059252e-03]]\n",
      "grad-0 is [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "grad-1 is [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "Timestamp #1100\n",
      "rewards.tail 971    0.012944\n",
      "972    0.000578\n",
      "973    0.001801\n",
      "Name: y, dtype: float32\n",
      "Reward is -1\n",
      "y=0    0.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "Name: y, dtype: float32\n",
      "py=[[-0.01675762]\n",
      " [ 0.00074415]\n",
      " [-0.01008121]]\n",
      "grad-0 is [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "grad-1 is [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "Timestamp #1200\n",
      "rewards.tail 973   -0.002433\n",
      "974    0.004971\n",
      "975   -0.043619\n",
      "Name: y, dtype: float32\n",
      "Reward is -0.06722142736796209\n",
      "y=0    0.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "Name: y, dtype: float32\n",
      "py=[[-0.00774193]\n",
      " [ 0.00074317]\n",
      " [-0.00278633]]\n",
      "grad-0 is [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "grad-1 is [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "Timestamp #1300\n",
      "rewards.tail 967    0.013947\n",
      "968   -0.004828\n",
      "969    0.001617\n",
      "Name: y, dtype: float32\n",
      "Reward is -1\n",
      "y=0    0.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "Name: y, dtype: float32\n",
      "py=[[-0.00355411]\n",
      " [ 0.01175342]\n",
      " [-0.01747834]]\n",
      "grad-0 is [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "grad-1 is [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "Timestamp #1400\n",
      "rewards.tail 978   -0.014210\n",
      "979   -0.003905\n",
      "980   -0.027020\n",
      "Name: y, dtype: float32\n",
      "Reward is -1\n",
      "y=0    0.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "Name: y, dtype: float32\n",
      "py=[[-0.01349853]\n",
      " [ 0.00455079]\n",
      " [ 0.01830468]]\n",
      "grad-0 is [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "grad-1 is [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "Timestamp #1500\n",
      "rewards.tail 1024    0.044518\n",
      "1025    0.093498\n",
      "1026    0.051827\n",
      "Name: y, dtype: float32\n",
      "Reward is -1\n",
      "y=0    0.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "Name: y, dtype: float32\n",
      "py=[[ 0.0865609 ]\n",
      " [-0.018748  ]\n",
      " [ 0.05322869]]\n",
      "grad-0 is [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "grad-1 is [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "Timestamp #1600\n",
      "rewards.tail 1060    0.008592\n",
      "1061   -0.000332\n",
      "1062   -0.026541\n",
      "Name: y, dtype: float32\n",
      "Reward is -1\n",
      "y=0    0.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "Name: y, dtype: float32\n",
      "py=[[-0.00418377]\n",
      " [-0.02375708]\n",
      " [-0.05870634]]\n",
      "grad-0 is [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "grad-1 is [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "Timestamp #1700\n",
      "rewards.tail 1083   -1.027922\n",
      "1084   -0.995981\n",
      "1085   -0.995966\n",
      "Name: y, dtype: float32\n",
      "Reward is -1\n",
      "y=0    1.0\n",
      "1    1.0\n",
      "2    1.0\n",
      "Name: y, dtype: float32\n",
      "py=[[ 0.00490831]\n",
      " [-0.00466271]\n",
      " [ 0.00759365]]\n",
      "grad-0 is [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "grad-1 is [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "Timestamp #1800\n",
      "rewards.tail 1081   -0.982740\n",
      "1082   -1.009995\n",
      "1083   -1.021448\n",
      "Name: y, dtype: float32\n",
      "Reward is -1\n",
      "y=0    1.0\n",
      "1    1.0\n",
      "2    1.0\n",
      "Name: y, dtype: float32\n",
      "py=[[-0.01450168]\n",
      " [-0.00306071]\n",
      " [-0.03311516]]\n",
      "grad-0 is [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "grad-1 is [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "env = kagglegym.make()\n",
    "# Get first observation\n",
    "observation = env.reset()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset() # Obtain an initial observation of the environment\n",
    "\n",
    "    # Reset the gradient placeholder. We will collect gradients in \n",
    "    # gradBuffer until we are ready to update our policy network. \n",
    "    gradBuffer = sess.run(tvars)\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    print(\"gradBuffer\", len(gradBuffer), len(gradBuffer[0]), len(gradBuffer[1]))\n",
    "\n",
    "    rewards = []\n",
    "    done = False\n",
    "    while not done:\n",
    "        x = observation.features.fillna(.0) #.head(1)\n",
    "        # log\n",
    "        timestamp = x[\"timestamp\"][0]\n",
    "        #timestamp = 0\n",
    "        debug = Debug(timestamp)\n",
    "        #debug.log(x.tail(3))\n",
    "        debug.log(\"Timestamp #{}\".format(timestamp))\n",
    "\n",
    "        # Run the policy network and get an action to take. \n",
    "        y = sess.run(probability,feed_dict={observations: x})\n",
    "        action = observation.target\n",
    "        action[\"y\"] = y\n",
    "        # step the environment and get new measurements\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        # get perfect action for training\n",
    "        perfect_action = df[df[\"timestamp\"] == timestamp][[\"id\", \"y\"]].reset_index(drop=True)\n",
    "        \n",
    "        # calculate rewards\n",
    "        pa = perfect_action\n",
    "        y = action[\"y\"]\n",
    "        #debug.log(\"Perfect action {}\".format(pa.tail(3)))\n",
    "        rewards = pa[\"y\"] - y #.fillna(.0)\n",
    "        debug.log(\"rewards.tail {}\".format(rewards.tail(3)))\n",
    "        rewards = -np.abs(rewards) #* 10\n",
    "        #rewards = np.nan_to_num(rewards)\n",
    "        #debug.log(\"rewards.shape {0}\".format(rewards.shape))\n",
    "        \n",
    "        # record reward (has to be done after we call step() to get reward for previous action)\n",
    "        #rewards = y * reward\n",
    "        rewards = np.vstack(rewards)\n",
    "        \n",
    "        # log\n",
    "        debug.log(\"Reward is {}\".format(reward))\n",
    "        \n",
    "        # compute the discounted reward backwards through time\n",
    "        discounted_epr = discount_rewards(rewards)\n",
    "        # size the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "        discounted_epr -= np.mean(discounted_epr)\n",
    "        discounted_epr /= np.std(discounted_epr)\n",
    "        # Get the gradient for this episode, and save it in the gradBuffer\n",
    "        py = pa[\"y\"]\n",
    "        py = py.reshape([py.shape[0],1])\n",
    "        debug.log(\"y={}\".format(y[0:3]))\n",
    "        debug.log(\"py={}\".format(py[0:3]))\n",
    "        tGrad = sess.run(newGrads,feed_dict={observations: x, input_y: py, advantages: discounted_epr})\n",
    "        for ix,grad in enumerate(tGrad):\n",
    "            debug.log(\"grad-{0} is {1}\".format(str(ix), grad[0:3]))\n",
    "            gradBuffer[ix] += grad\n",
    "        \n",
    "        # If we have completed enough episodes, then update the policy network with our gradients.\n",
    "        sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W2Grad:gradBuffer[1]})\n",
    "        for ix,grad in enumerate(gradBuffer):\n",
    "            gradBuffer[ix] = grad * 0\n",
    "\n",
    "        #if timestamp > 920:\n",
    "        #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp #1000\n",
      "Timestamp #1100\n",
      "Timestamp #1200\n",
      "Timestamp #1300\n",
      "Timestamp #1400\n",
      "Timestamp #1500\n",
      "Timestamp #1600\n",
      "Timestamp #1700\n",
      "Timestamp #1800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'public_score': -0.0083462397338969572}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = kagglegym.make()\n",
    "observation = env.reset()\n",
    "\n",
    "while True:\n",
    "    target = observation.target\n",
    "    timestamp = observation.features[\"timestamp\"][0]\n",
    "    if timestamp % 100 == 0:\n",
    "        print(\"Timestamp #{}\".format(timestamp))\n",
    "\n",
    "    observation, reward, done, info = env.step(target)\n",
    "    if done:        \n",
    "        break\n",
    "        \n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "class agent():\n",
    "    def __init__(self, lr, s_size,a_size,h_size):\n",
    "        #These lines established the feed-forward part of the network. The agent takes a state and produces an action.\n",
    "        self.state_in= tf.placeholder(shape=[None,s_size],dtype=tf.float32)\n",
    "        hidden = slim.fully_connected(self.state_in,h_size,biases_initializer=None,activation_fn=tf.nn.relu)\n",
    "        self.output = slim.fully_connected(hidden,a_size,activation_fn=tf.nn.softmax,biases_initializer=None)\n",
    "        self.chosen_action = tf.argmax(self.output,1)\n",
    "\n",
    "        #The next six lines establish the training proceedure. We feed the reward and chosen action into the network\n",
    "        #to compute the loss, and use it to update the network.\n",
    "        self.reward_holder = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        \n",
    "        self.indexes = tf.range(0, tf.shape(self.output)[0]) * tf.shape(self.output)[1] + self.action_holder\n",
    "        self.responsible_outputs = tf.gather(tf.reshape(self.output, [-1]), self.indexes)\n",
    "\n",
    "        self.loss = -tf.reduce_mean(tf.log(self.responsible_outputs)*self.reward_holder)\n",
    "        \n",
    "        tvars = tf.trainable_variables()\n",
    "        self.gradient_holders = []\n",
    "        for idx,var in enumerate(tvars):\n",
    "            placeholder = tf.placeholder(tf.float32,name=str(idx)+'_holder')\n",
    "            self.gradient_holders.append(placeholder)\n",
    "        \n",
    "        self.gradients = tf.gradients(self.loss,tvars)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders,tvars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# TODO implement arbitrary actions from https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb\n",
    "\n",
    "tf.reset_default_graph() #Clear the Tensorflow graph.\n",
    "myAgent = agent(lr=1e-2,s_size=4,a_size=2,h_size=8) #Load the agent.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
